# 指定Agent的组件名称
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2
 
# 指定Flume source(要监听的路径)
a1.sources.r1.type = com.yss.source.spooldir.SpoolDirectorySource
a1.sources.r1.spoolDir= /data/gz_interface/
a1.sources.r1.ignorePattern= ^(.*(\.xls[x|d]$))$
#指定Xml文件的节点key值,不指定默认Security
a1.sources.r1.xmlNode = Security
#指定csv文件的分隔符,不指定默认逗号分隔
a1.sources.r1.csvSeparator = ,
#指定是否需要文件的相对路径,默认ture
a1.sources.r1.fileHeader = true
#指定获取文件相对路径的key值,默认值fileName
a1.sources.r1.fileHeaderKey = fileName
#指定当前行的行号的key值,默认值是currentRecord
a1.sources.r1.currentLine = currentRecord
#指定是否递归监控,默认值是false
a1.sources.r1.recursiveDirectorySearch = true
 
# 指定Flume hdfs sink
a1.sinks.k1.type = com.yss.sink.hdfs.HDFSEventSink
a1.sinks.k1.hdfs.path = /yss/guzhi/ws/
a1.sinks.k1.hdfs.filePrefix = %{fileName}
a1.sinks.k1.hdfs.fileSuffix = .csv
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.fileType  = DataStream
a1.sinks.k1.hdfs.writeFormat  = Text
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.idleTimeout  = 5
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.useLocalTimeStamp = true

# 指定Flume kafka sink
#a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink
#a1.sinks.k2.kafka.topic = ws_test
#a1.sinks.k2.kafka.bootstrap.servers = bj-rack001-hadoop004:6667,bj-rack001-hadoop002:6667,bj-rack001-hadoop003:6667
#a1.sinks.k2.kafka.flumeBatchSize = 20
#a1.sinks.k2.useFlumeEventFormat = true


 
# 指定Flume hdfs channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
a1.channels.c1.byteCapacityBufferPercentage = 20
a1.channels.c1.byteCapacity = 800000


# 指定Flume kafka channel
#a1.channels.c2.type = memory
#a1.channels.c2.capacity = 1000
#a1.channels.c2.transactionCapacity = 100
#a1.channels.c2.byteCapacityBufferPercentage = 20
#a1.channels.c2.byteCapacity = 800000


 
# 绑定source和sink到channel上
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
#a1.sinks.k2.channel = c2

